{
  "cells": [
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute SESAME inverse solution on evoked data in volume source space\n\n\nIn this example we shall compute SESAME inverse solution on evoked data in\na volume source space. Data are taken from the MNE-Python\n`sample <https://mne.tools/stable/generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path>`_\ndataset and correspond to the response to an auditory stimulus.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "# Authors: Gianvittorio Luria <luria@dima.unige.it>\n#          Sara Sommariva <sommariva@dima.unige.it>\n#          Alberto Sorrentino <sorrentino@dima.unige.it>\n#\n# License: BSD (3-clause)\n\n# sphinx_gallery_thumbnail_number = 4\n\nfrom os import path as op\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nilearn.plotting import plot_stat_map\nfrom nilearn.image import index_img\nfrom mne import read_forward_solution, pick_types_forward, read_evokeds, \\\n    read_trans, head_to_mri\nfrom mne.datasets import sample\nfrom mne.label import _n_colors\nfrom sesameeg import Sesame\nimport time\n\n\ndata_path = sample.data_path()\nsubject = 'sample'\nsubjects_dir = op.join(data_path, 'subjects')\nfname_fwd = op.join(data_path, 'MEG', subject,\n                    'sample_audvis-meg-vol-7-fwd.fif')\nfname_trans = op.join(data_path, 'MEG', subject,\n                      'sample_audvis_raw-trans.fif')\nfname_t1 = op.join(data_path , 'subjects', subject, 'mri', 'T1.mgz')\nfname_evoked = op.join(data_path, 'MEG', subject, 'sample_audvis-ave.fif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the  mri-to-head coordinates transformation matrix, the forward solution\n$\\textbf{G}$ and the evoked data $\\textbf{y}$.\nThe forward solution also defines the employed brain discretization which, in this example,\ncomprises the whole brain volume.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "# Transformation matrix\ntrans = read_trans(fname_trans)\n\n# Choose sensor type\nmeg_sensor_type = True  # All meg sensors will be included\neeg_sensor_type = False\n\n# Forward solution\nfwd = read_forward_solution(fname_fwd, exclude='bads')\nfwd = pick_types_forward(fwd, meg=meg_sensor_type,\n                         eeg=eeg_sensor_type, ref_meg=False)\n\n# Evoked Data\ncondition = 'Left Auditory'\nevoked = read_evokeds(fname_evoked, condition=condition, baseline=(None, 0))\nevoked = evoked.pick_types(meg=meg_sensor_type,\n                           eeg=eeg_sensor_type, exclude='bads')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the parameters.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "time_min, time_max = 0.045, 0.135  # Select N100m\nsubsample = None\nsample_min, sample_max = evoked.time_as_index([time_min, time_max],\n                                              use_rounding=True)\n# To accelerate the run time of this example, we use a small number of\n# particles. We recall that the parameter ``n_parts`` represents, roughly speaking,\n# the number of candidate solutions that are tested in the Monte Carlo procedure;\n# larger values yield in principle more accurate reconstructions but also entail a\n# higher computational cost. Setting the value to about a hundred seems to represent\n# a good trade\u2013off.\nn_parts = 10\n# If None, noise_std and dip_mom_std will be estimated by SESAME.\nnoise_std = None\ndip_mom_std = None\n\n\nnoise_cov = None\n# You can make SESAME pre-whiten the data by providing a noise covariance\n# from mne import read_cov\n# fname_cov = op.join(sample.data_path(), 'MEG', subject,\n#                    'sample_audvis-cov.fif')\n# noise_cov = read_cov(fname_cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the selected data.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "lst = evoked.plot_joint(show=False)\nfor fig in lst:\n    ax = fig.get_axes()\n    ax[0].axvline(time_min, color='r', linewidth=2.0)\n    ax[0].axvline(time_max, color='r', linewidth=2.0)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply SESAME.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "_sesame = Sesame(fwd, evoked, n_parts=n_parts, noise_std=noise_std,\n                 top_min=time_min, top_max=time_max, dip_mom_std=dip_mom_std,\n                 hyper_q=True, noise_cov=noise_cov, subsample=subsample)\ntime_start = time.time()\n_sesame.apply_sesame()\ntime_elapsed = (time.time() - time_start)\nprint('    Estimated number of sources: {0}'.format(_sesame.est_n_dips[-1]))\nprint('    Estimated source locations: {0}'.format(_sesame.est_locs[-1]))\nprint('    Total computation time: {0}'.format(time_elapsed))\n\n# Compute goodness of fit\ngof = _sesame.goodness_of_fit()\nprint('    Goodness of fit with the recorded data: {0}%'.format(round(gof, 4) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize amplitude of the estimated sources as function of time.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "est_n_dips = _sesame.est_n_dips[-1]\nest_locs = _sesame.est_locs[-1]\n\ntimes = evoked.times[_sesame.s_min:_sesame.s_max+1]\namplitude = np.array([np.linalg.norm(_sesame.est_dip_moms[:, 3*i_d:3 * (i_d + 1)],\n                                     axis=1) for i_d in range(est_n_dips)])\ncolors = _n_colors(est_n_dips)\nplt.figure()\nfor idx, amp in enumerate(amplitude):\n    plt.plot(times, 1e9*amp, color=colors[idx], linewidth=2)\nplt.xlabel('Time (s)')\nplt.ylabel('Source amplitude (nAm)')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the posterior map of the dipoles' location\n$p(r| \\textbf{y}, 2)$ as an overlay onto the MRI.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "stc = _sesame.compute_stc(subject)\n\npeak_vertex, peak_time = stc.get_peak(vert_as_index=True,\n                                      time_as_index=True)\npeak_pos = fwd['source_rr'][peak_vertex]\npeak_mri_pos = head_to_mri(peak_pos, mri_head_t=trans,\n                           subject=subject, subjects_dir=subjects_dir)\n\n_time = stc.times[-1]\nstc.crop(_time-1, _time)\n\nimg = stc.as_volume(fwd['src'], mri_resolution=True)\nplot_stat_map(index_img(img, -1), fname_t1, threshold=0.001, cut_coords=peak_mri_pos)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save results.\n\n"
      ]
    },
    {
      "outputs": [],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "source": [
        "# You can save SESAME result in an HDF5 file with:\n# _sesame.save_h5(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)\n\n# You can save SESAME result in a Pickle file with:\n# _sesame.save_pkl(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)"
      ]
    }
  ],
  "nbformat": 4,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    },
    "language_info": {
      "version": "3.5.2",
      "nbconvert_exporter": "python",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "pygments_lexer": "ipython3",
      "file_extension": ".py",
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat_minor": 0
}