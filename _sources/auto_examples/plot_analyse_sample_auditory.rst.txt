.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_plot_analyse_sample_auditory.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_analyse_sample_auditory.py:


==============================================
Compute SESAME inverse solution on evoked data
==============================================

In this example we shall apply SESAME on an evoked dataset,
corresponding to the response to an auditory stimulus. Data are taken from the MNE-Python
`sample <https://mne.tools/stable/generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path>`_
dataset.


.. code-block:: default

    # Authors: Gianvittorio Luria <luria@dima.unige.it>
    #          Sara Sommariva <sommariva@dima.unige.it>
    #          Alberto Sorrentino <sorrentino@dima.unige.it>
    #
    # License: BSD (3-clause)

    # sphinx_gallery_thumbnail_number = 3

    from os import path as op
    import numpy as np
    import matplotlib.pyplot as plt
    from mayavi import mlab

    from mne.datasets import sample
    from mne import read_forward_solution, pick_types_forward, read_evokeds
    from mne.label import _n_colors

    from sesameeg import Sesame


    data_path = sample.data_path()
    subject = 'sample'
    subjects_dir = op.join(data_path, 'subjects')
    fname_fwd = op.join(data_path, 'MEG', subject,
                        'sample_audvis-meg-eeg-oct-6-fwd.fif')
    fname_evoked = op.join(data_path, 'MEG', subject, 'sample_audvis-ave.fif')







Load the forward solution  :math:`\textbf{G}`  and the evoked data
:math:`\textbf{y}`.
The forward solution also defines the employed brain discretization.


.. code-block:: default

    meg_sensor_type = True  # All MEG sensors will be included
    eeg_sensor_type = False

    # Forward solution
    fwd = read_forward_solution(fname_fwd, exclude='bads')
    fwd = pick_types_forward(fwd, meg=meg_sensor_type,
                             eeg=eeg_sensor_type, ref_meg=False)

    # Evoked Data
    condition = 'Left Auditory'
    evoked = read_evokeds(fname_evoked, condition=condition, baseline=(None, 0))
    evoked = evoked.pick_types(meg=meg_sensor_type,
                               eeg=eeg_sensor_type, exclude='bads')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Reading forward solution from /home/gv/mne_data/MNE-sample-data/MEG/sample/sample_audvis-meg-eeg-oct-6-fwd.fif...
        Reading a source space...
        Computing patch statistics...
        Patch information added...
        Distance information added...
        [done]
        Reading a source space...
        Computing patch statistics...
        Patch information added...
        Distance information added...
        [done]
        2 source spaces read
        Desired named matrix (kind = 3523) not available
        Read MEG forward solution (7498 sources, 306 channels, free orientations)
        Desired named matrix (kind = 3523) not available
        Read EEG forward solution (7498 sources, 60 channels, free orientations)
        MEG and EEG forward solutions combined
        Source spaces transformed to the forward solution coordinate frame
        364 out of 366 channels remain after picking
        305 out of 364 channels remain after picking
    Reading /home/gv/mne_data/MNE-sample-data/MEG/sample/sample_audvis-ave.fif ...
        Read a total of 4 projection items:
            PCA-v1 (1 x 102) active
            PCA-v2 (1 x 102) active
            PCA-v3 (1 x 102) active
            Average EEG reference (1 x 60) active
        Found the data of interest:
            t =    -199.80 ...     499.49 ms (Left Auditory)
            0 CTF compensation matrices available
            nave = 55 - aspect type = 100
    Projections have already been applied. Setting proj attribute to True.
    Applying baseline correction (mode: mean)



Define the parameters.


.. code-block:: default

    time_min, time_max = 0.045, 0.135  # Select N100m
    subsample = None
    sample_min, sample_max = evoked.time_as_index([time_min, time_max],
                                                  use_rounding=True)

    # To accelerate the run time of this example, we use a small number of
    # particles. We recall that the parameter ``n_parts`` represents, roughly speaking,
    # the number of candidate solutions that are tested in the Monte Carlo procedure;
    # larger values yield in principle more accurate reconstructions but also entail a
    # higher computational cost. Setting the value to about a hundred seems to represent
    # a good tradeâ€“off.
    n_parts = 10
    # If None, noise_std and dip_mom_std will be estimated by SESAME.
    noise_std = None
    dip_mom_std = None


    noise_cov = None
    # You can make SESAME pre-whiten the data by providing a noise covariance
    # from mne import read_cov
    # fname_cov = op.join(sample.data_path(), 'MEG', subject,
    #                    'sample_audvis-cov.fif')
    # noise_cov = read_cov(fname_cov)







Visualize the selected data.


.. code-block:: default


    fig = evoked.plot(show=False)
    for ax in fig.get_axes():
        ax.axvline(time_min, color='r', linewidth=2.0)
        ax.axvline(time_max, color='r', linewidth=2.0)
    plt.show()




.. image:: /auto_examples/images/sphx_glr_plot_analyse_sample_auditory_001.png
    :class: sphx-glr-single-img




Apply SESAME.


.. code-block:: default

    _sesame = Sesame(fwd, evoked, n_parts=n_parts, noise_std=noise_std,
                     top_min=time_min, top_max=time_max, dip_mom_std=dip_mom_std,
                     hyper_q=True, noise_cov=noise_cov, subsample=subsample)
    _sesame.apply_sesame()

    print('    Estimated number of sources: {0}'.format(_sesame.est_n_dips[-1]))
    print('    Estimated source locations: {0}'.format(_sesame.est_locs[-1]))

    # Compute goodness of fit
    gof = _sesame.goodness_of_fit()
    print('    Goodness of fit with the recorded data: {0}%'.format(round(gof, 4) * 100))

    # Compute source dispersion
    sd = _sesame.source_dispersion()
    print('    Source Dispersion: {0} mm'.format(round(sd, 2)))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Computing inverse operator with 305 channels.
        305 out of 305 channels remain after picking
    Forward model with free source orientation.
    Computing neighbours matrix [done]
    Computing neighbours probabilities...[done]
    Analyzing data from 0.045 s to 0.1349 s
    Estimating dipole moment std...[done]
     Estimated dipole moment std: 3.2335e-08
    Sampling hyperprior for dipole moment std.
    Estimating noise std...[done]
     Estimated noise std: 3.9983e-12
    Computing inverse solution. This will take a while...
    Estimated dipole strength variance: 3.042870043769498e-08
    [done]
        Estimated number of sources: 2
        Estimated source locations: [5624 1745]
        Goodness of fit with the recorded data: 69.92%
        Source Dispersion: 0.52 mm



Visualize the amplitude of the estimated sources as function of time.


.. code-block:: default

    est_n_dips = _sesame.est_n_dips[-1]
    est_locs = _sesame.est_locs[-1]

    times = evoked.times[_sesame.s_min:_sesame.s_max+1]
    amplitude = np.array([np.linalg.norm(_sesame.est_dip_moms[:, 3*i_d:3 * (i_d + 1)],
                                         axis=1) for i_d in range(est_n_dips)])
    colors = _n_colors(est_n_dips)
    plt.figure()
    for idx, amp in enumerate(amplitude):
        plt.plot(1e3*times, 1e9*amp, color=colors[idx], linewidth=2)
    plt.xlabel('Time (ms)')
    plt.ylabel('Source amplitude (nAm)')
    plt.show()




.. image:: /auto_examples/images/sphx_glr_plot_analyse_sample_auditory_002.png
    :class: sphx-glr-single-img




Visualize the posterior map of the dipoles' location
:math:`p(r| \textbf{y}, 2)` and the estimated sources on the inflated brain.


.. code-block:: default

    stc = _sesame.compute_stc(subject)
    clim = dict(kind='value', lims=[1e-4, 1e-1, 1])
    brain = stc.plot(subject, surface='inflated', hemi='split', clim=clim,
                     time_label=' ', subjects_dir=subjects_dir, size=(1000, 600))
    nv_lh = stc.vertices[0].shape[0]
    for idx, loc in enumerate(est_locs):
        if loc < nv_lh:
            brain.add_foci(stc.vertices[0][loc], coords_as_verts=True,
                           hemi='lh', color=colors[idx], scale_factor=0.3)
        else:
            brain.add_foci(stc.vertices[1][loc-nv_lh], coords_as_verts=True,
                           hemi='rh', color=colors[idx], scale_factor=0.3)

    mlab.show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /auto_examples/images/sphx_glr_plot_analyse_sample_auditory_003.png
            :class: sphx-glr-multi-img

    *

      .. image:: /auto_examples/images/sphx_glr_plot_analyse_sample_auditory_004.png
            :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Surface stc computed.
    Using mayavi 3d backend.




Save results.


.. code-block:: default


    # You can save SESAME result in an HDF5 file with:
    # _sesame.save_h5(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)

    # You can save SESAME result in a Pickle file with:
    # _sesame.save_pkl(save_fname, sbj=subject, data_path=fname_evoked, fwd_path=fname_fwd)







.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  55.208 seconds)


.. _sphx_glr_download_auto_examples_plot_analyse_sample_auditory.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_analyse_sample_auditory.py <plot_analyse_sample_auditory.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_analyse_sample_auditory.ipynb <plot_analyse_sample_auditory.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
